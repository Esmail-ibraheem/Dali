
# Model Architecture:

![[Pasted image 20240322144701.png]]

Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional _latent_ space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: **in latent diffusion the model is trained to generate latent (compressed) representations of the images.**

![[Pasted image 20240322150727.png]]
> [!Note] The Pipeline of the model: [[#What’s their role in the Stable diffusion pipeline]]

There are three main components in latent diffusion.

1. An autoencoder (VAE).
	An autoencoder contains two parts -  
	1. `Encoder` takes an image as input and converts it into a low dimensional latent representation  
	2. `Decoder` takes the latent representation and converts it back into an image
	
	![[variational-autoencoder.jpg]]

	 ![[Pasted image 20240322160146.png]]
	 As we can see above, the Encoder acts like a compressor that squishes the image into lower dimensions and the decoder recreates the original image back from the compressed version.

```python
p = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg') 
img = load_image(p) 
print(f"Dimension of this image: {np.array(img).shape}") img	
```
	
```
Dimension of this image: (512, 512, 3)
```
	
	![[Pasted image 20240322160606.png]]
	Now let’s compress this image by using the VAE encoder, we will be using the `pil_to_latents` helper function.

```python
latent_img = pil_to_latents(img) 
print(f"Dimension of this latent representation: {latent_img.shape}")
```

```
Dimension of this latent representation: torch.Size([1, 4, 64, 64])
```

As we can see how the VAE compressed a 3 x 512 x 512 dimension image into a 4 x 64 x 64 image. That’s a compression ratio of 48x! Let’s visualize these four channels of latent representations.

```python
fig, axs = plt.subplots(1, 4, figsize=(16, 4)) []for c in range(4): [])axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')
```

![[Pasted image 20240322161151.png]]
	This latent representation in theory should capture a lot of information about the original image. Let’s use the decoder on this representation to see what we get back. For this, we will use the `latents_to_pil` helper function.
	
```python
decoded_img = latents_to_pil(latent_img) 
decoded_img[0]
```

	![[Pasted image 20240322161321.png]]
	As we can see from the figure above VAE decoder was able to recover the original image from a 48x compressed latent representation. That’s impressive!
	
>[!Note] If you look closely at the decoded image, it’s not the same as the original image, notice the difference around the eyes. That’s why VAE encoder/decoder is not a lossless compression.
	
2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).
	![[u-net.png]]
	The U-Net model takes two inputs -  
	1. `Noisy latent` or `Noise`- Noisy latents are latents produced by a VAE encoder (in case an initial image is provided) with added noise or it can take pure noise input in case we want to create a random new image based solely on a textual description  
	2. `Text embeddings` - CLIP-based embedding generated by input textual prompts
	
	The output of the U-Net model is the predicted noise residual which the input noisy latent contains. In other words, it predicts the noise which is subtracted from the noisy latents to return the original de-noised latents.
	![[Pasted image 20240322161846.png]]
```python
from diffusers import UNet2DConditionModel, LMSDiscreteScheduler [][]
## Initializing a scheduler 
scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000) 
## Setting number of sampling steps 
scheduler.set_timesteps(51)

unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")
```

As you may have noticed from code above, we not only imported `unet` but also a `scheduler`. The purpose of a `schedular` is to determine how much noise to add to the latent at a given step in the diffusion process. Let’s visualize the schedular function -
	![[Pasted image 20240322162725.png]]
	The diffusion process follows this sampling schedule where we start with high noise and gradually denoise the image. Let’s visualize this process -
	![[Pasted image 20240322162757.png]]
	Let’s see how a U-Net removes the noise from the image. Let’s start by adding some noise to the image.
	
	![[Pasted image 20240322162822.png]]
	
```python
prompt = [""]

text_input = tokenizer(prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")

with torch.no_grad(): 
text_embeddings = text_encoder(text_input.input_ids.to("cuda"))[0]

latent_model_input =torch.cat([encoded_and_noised.to("cuda").float()]).half() with torch.no_grad(): 
noise_pred = unet(latent_model_input, 40, encoder_hidden_states=text_embeddings)["sample"]

latents_to_pil(encoded_and_noised- noise_pred)[0]

```
	
	![[Pasted image 20240322163243.png]]
	As we can see above the U-Net output is clearer than the original noisy input passed.
	
2. A text-encoder, _e.g._ [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).

	Any machine learning model doesn’t understand text data. For any model to understand text data, we need to convert this text into numbers that hold the meaning of the text, referred to as `embeddings`. The process of converting a text to a number can be broken down into two parts -  
		1. **_Tokenizer_** - Breaking down each word into sub-words and then using a lookup table to convert them into a number  
		2. **_Token-To-Embedding Encoder_** - Converting those numerical sub-words into a representation that contains the representation of that text
	![[Pasted image 20240322153059.png]]
	
   ![[Pasted image 20240322153247.png]]

	Stable diffusion only uses a CLIP trained encoder for the conversion of text to embeddings. This becomes one of the inputs to the U-net. On a high level, CLIP uses an image encoder and text encoder to create embeddings that are similar in latent space. This similarity is more precisely defined as a [Contrastive objective](https://arxiv.org/abs/1807.03748).

```python
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16) 

text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")

prompt = ["a dog wearing hat"] []tok =tokenizer(prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt") []print(tok.input_ids.shape) []tok

```


```
torch.Size([1, 77])
{'input_ids': tensor([[49406,   320,  1929,  3309,  3801, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,
         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0]])}
```

### What’s their role in the Stable diffusion pipeline

Latent diffusion uses the U-Net to gradually subtract noise in the latent space over several steps to reach the desired output. With each step, the amount of noise added to the latents is reduced till we reach the final de-noised output. U-Nets were first introduced by [this paper](https://arxiv.org/abs/1505.04597) for Biomedical image segmentation. The U-Net has an encoder and a decoder which are comprised of ResNet blocks. The stable diffusion U-Net also has cross-attention layers to provide them with the ability to condition the output based on the text description provided. The Cross-attention layers are added to both the encoder and the decoder part of the U-Net usually between ResNet blocks. You can learn more about this U-Net architecture [here](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).

![[Pasted image 20240322170805.png]]
The stable diffusion model takes the textual input and a seed. The textual input is then passed through the CLIP model to generate textual embedding of size 77x768 and the seed is used to generate Gaussian noise of size 4x64x64 which becomes the first latent image representation.

Next, the U-Net iteratively denoises the random latent image representations while conditioning on the text embeddings. The output of the U-Net is predicted noise residual, which is then used to compute conditioned latents via a scheduler algorithm. This process of denoising and text conditioning is repeated N times (We will use 50) to retrieve a better latent image representation. Once this process is complete, the latent image representation (4x64x64) is decoded by the VAE decoder to retrieve the final output image (3x512x512).

---

